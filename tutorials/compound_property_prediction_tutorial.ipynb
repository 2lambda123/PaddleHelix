{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tuorial, we will go through how to run a Graph Neural Network (GNN) model for compound property prediction. In particular, we will demonstrate how to pretrain it and how to finetune in the downstream tasks.\n",
    "\n",
    "#  Part I: Pretraining\n",
    "\n",
    "In this part, we will show how to pretrain a compound GNN model. The pretraining skills here are adapted from the work of pretrain gnns, including attribute masking, context prediction and supervised pretraining.\n",
    "\n",
    "Visit `pretrain_attrmask.py`, `pretrain_contextpred.py` and `pretrain_supervised.py` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, os.getcwd() + \"/..\")\n",
    "os.chdir(\"../apps/pretrained_compound/pretrain_gnns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pahelix framework is build upon PaddlePaddle, which is a deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "from paddle.fluid.incubate.fleet.collective import fleet\n",
    "from pahelix.datasets import load_zinc_dataset\n",
    "from pahelix.featurizer import PreGNNAttrMaskFeaturizer\n",
    "from pahelix.utils.compound_tools import CompoundConstants\n",
    "from pahelix.model_zoo import PreGNNAttrmaskModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to paddle static graph mode.\n",
    "paddle.enable_static()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the static graph\n",
    "Basically we build the static graph with Paddle Programs and Executors. Here, we use `model_config` to hold the model configurations. `PreGNNAttrmaskModel` is an unsupervised pretraining model which randomly masks the atom type of some node and then use the masked atom type as the prediction targets. Meanwhile, we use Adam optimizer and set the lr to be 0.001.\n",
    "\n",
    "To use the GPU for training, please uncomment the `fluid.CUDAPlace(0)`. While the `fluid.CPUPlace()` is for CPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var mean_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)\n"
     ]
    }
   ],
   "source": [
    "model_config = {\n",
    "    \"dropout_rate\": 0.5,# dropout rate\n",
    "    \"gnn_type\": \"gin\",  # other choices like \"gat\", \"gcn\".\n",
    "    \"layer_num\": 5,     # the number of gnn layers.\n",
    "}\n",
    "train_prog = fluid.Program()\n",
    "startup_prog = fluid.Program()\n",
    "with fluid.program_guard(train_prog, startup_prog):\n",
    "    with fluid.unique_name.guard():\n",
    "        model = PreGNNAttrmaskModel(model_config=model_config)\n",
    "        model.forward()\n",
    "        opt = fluid.optimizer.Adam(learning_rate=0.001)\n",
    "        opt.minimize(model.loss)\n",
    "\n",
    "exe = fluid.Executor(fluid.CPUPlace())\n",
    "# exe = fluid.Executor(fluid.CUDAPlace(0))\n",
    "exe.run(startup_prog)\n",
    "print(model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading and feature extraction\n",
    "`PreGNNAttrMaskFeaturizer` is used along with `PreGNNAttrmaskModel`. It inherits from the super class `Featurizer` which is used for feature extractions. The `Featurizer` has two functions: `gen_features` for converting from a single raw smiles to a single graph data, `collate_fn` for aggregating a sublist of graph data into a big batch.\n",
    "The zinc dataset is used as the pretraining dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset num: 1000\n"
     ]
    }
   ],
   "source": [
    "featurizer = PreGNNAttrMaskFeaturizer(\n",
    "        model.graph_wrapper, \n",
    "        atom_type_num=len(CompoundConstants.atom_num_list),\n",
    "        mask_ratio=0.15)\n",
    "dataset = load_zinc_dataset(\"../../../data/chem_dataset/zinc_standard_agent/raw\", featurizer=featurizer)\n",
    "print(\"dataset num: %s\" % (len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start train\n",
    "Now we train the attrmask model for 2 epochs for demostration purposes. The data loading process is accelerated with 4 processors. Then the pretrained model is saved to \"./model/pretrain_attrmask\", which will serve as the initial model of the downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 train/loss:4.3884435\n",
      "epoch:1 train/loss:1.53111\n"
     ]
    }
   ],
   "source": [
    "def train(exe, train_prog, model, dataset, featurizer):\n",
    "    data_gen = dataset.iter_batch(\n",
    "            batch_size=256, num_workers=4, shuffle=True, collate_fn=featurizer.collate_fn)\n",
    "    list_loss = []\n",
    "    for batch_id, feed_dict in enumerate(data_gen):\n",
    "        train_loss, = exe.run(train_prog, \n",
    "                feed=feed_dict, fetch_list=[model.loss], return_numpy=False)\n",
    "        list_loss.append(np.array(train_loss).mean())\n",
    "    return np.mean(list_loss)\n",
    "\n",
    "for epoch_id in range(2):\n",
    "    train_loss = train(exe, train_prog, model, dataset, featurizer)\n",
    "    print(\"epoch:%d train/loss:%s\" % (epoch_id, train_loss))\n",
    "fluid.io.save_params(exe, './model/pretrain_attrmask', train_prog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is about the pretraining steps,you can adjust as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream finetuning\n",
    "Below we will introduce how to use the pretrained model for the finetuning of downstream tasks.\n",
    "\n",
    "Visit `finetune.py` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pahelix.utils.paddle_utils import load_partial_params\n",
    "from pahelix.utils.splitters import \\\n",
    "    RandomSplitter, IndexSplitter, ScaffoldSplitter, RandomScaffoldSplitter\n",
    "from pahelix.datasets import *\n",
    "\n",
    "from model import DownstreamModel\n",
    "from featurizer import DownstreamFeaturizer\n",
    "from utils import calc_rocauc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downstream datasets are usually small and have different tasks. For example, the BBBP dataset is used for the predictions of the Blood-brain barrier permeability. The Tox21 dataset is used for the predictions of toxicity of compounds. Here we use the Tox21 dataset for demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53']\n"
     ]
    }
   ],
   "source": [
    "task_names = get_default_tox21_task_names()\n",
    "# task_names = get_default_sider_task_names()\n",
    "print(task_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the static graph\n",
    "Basically we build the static graph with Paddle Programs and Executors. Here, we use `model_config` to hold the model configurations. Note that the configurations of the model architecture should align with that of the pretraining model, otherwise the loading will fail. `DownstreamModel` is an supervised GNN model which predicts the tasks shown in `task_names`. Meanwhile, we use Adam optimizer and set the lr to be 0.001.\n",
    "\n",
    "To use the GPU for training, please uncomment the `fluid.CUDAPlace(0)`. While the `fluid.CPUPlace()` is for CPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = {\n",
    "    \"dropout_rate\": 0.5,# dropout rate\n",
    "    \"gnn_type\": \"gin\",  # other choices like \"gat\", \"gcn\".\n",
    "    \"layer_num\": 5,     # the number of gnn layers.\n",
    "    \"num_tasks\": len(task_names), # number of targets to predict for the downstream task.\n",
    "}\n",
    "train_prog = fluid.Program()\n",
    "startup_prog = fluid.Program()\n",
    "with fluid.program_guard(train_prog, startup_prog):\n",
    "    with fluid.unique_name.guard():\n",
    "        model = DownstreamModel(model_config=model_config)\n",
    "        model.forward()\n",
    "        test_prog = train_prog.clone(for_test=True)\n",
    "        adam = fluid.optimizer.Adam(learning_rate=0.001)\n",
    "        adam.minimize(model.loss)\n",
    "\n",
    "exe = fluid.Executor(fluid.CPUPlace())\n",
    "# exe = fluid.Executor(fluid.CUDAPlace(0))\n",
    "exe.run(startup_prog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained models\n",
    "Load the pretrained model in the pretraining phase.here we load the pretrain_attrmask as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load parameters from ./model/pretrain_attrmask.\n"
     ]
    }
   ],
   "source": [
    "load_partial_params(exe, './model/pretrain_attrmask', train_prog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading and feature extraction\n",
    "`DownstreamFeaturizer` is used along with `DownstreamModel`. It inherits from the super class `Featurizer` which is used for feature extractions. The `Featurizer` has two functions: `gen_features` for converting from a single raw smiles to a single graph data, `collate_fn` for aggregating a sublist of graph data into a big batch.\n",
    "\n",
    "The Tox21 dataset is used as the downstream dataset and we use `ScaffoldSplitter` to split the dataset into train/valid/test set. `ScaffoldSplitter` will firstly order the compounds according to Bemis-Murcko scaffold, then take the first `frac_train` proportion as the train set, the next `frac_valid` proportion as the valid set and the rest as the test set. `ScaffoldSplitter` can better evaluate the generalization ability of the model on out-of-distribution samples. Note that other splitters like `RandomSplitter`, `RandomScaffoldSplitter` and `IndexSplitter` is also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [10:36:42] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [10:36:53] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test num: 6264/783/784\n"
     ]
    }
   ],
   "source": [
    "featurizer = DownstreamFeaturizer(model.graph_wrapper)\n",
    "dataset = load_tox21_dataset(\n",
    "        \"../../../data/chem_dataset/tox21/raw\", task_names, featurizer=featurizer)\n",
    "# dataset = load_sider_dataset(\n",
    "#         \"../../../data/chem_dataset/sider/raw\", task_names, featurizer=featurizer)\n",
    "\n",
    "# splitter = RandomSplitter()\n",
    "splitter = ScaffoldSplitter()\n",
    "train_dataset, valid_dataset, test_dataset = splitter.split(\n",
    "        dataset, frac_train=0.8, frac_valid=0.1, frac_test=0.1)\n",
    "print(\"Train/Valid/Test num: %s/%s/%s\" % (\n",
    "        len(train_dataset), len(valid_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start train\n",
    "Now we train the attrmask model for 4 epochs for demostration purposes. Since each downstream task will contain more than one sub-task, the performance of the model is evaluated by the average roc-auc on all sub-tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 train/loss:0.503272\n",
      "epoch:0 val/auc:0.6493802688609531\n",
      "epoch:0 test/auc:0.6282874648082166\n",
      "epoch:1 train/loss:0.25345027\n",
      "epoch:1 val/auc:0.6470544426655516\n",
      "epoch:1 test/auc:0.6279795005837258\n",
      "epoch:2 train/loss:0.21940874\n",
      "epoch:2 val/auc:0.6548664758200556\n",
      "epoch:2 test/auc:0.6585136404994928\n",
      "epoch:3 train/loss:0.21591327\n",
      "epoch:3 val/auc:0.6972151038228583\n",
      "epoch:3 test/auc:0.7012427094328548\n"
     ]
    }
   ],
   "source": [
    "def train(exe, train_prog, model, train_dataset, featurizer):\n",
    "    data_gen = train_dataset.iter_batch(\n",
    "        batch_size=64, num_workers=4, shuffle=True, collate_fn=featurizer.collate_fn)\n",
    "    list_loss = []\n",
    "    for batch_id, feed_dict in enumerate(data_gen):\n",
    "        train_loss, = exe.run(train_prog, feed=feed_dict, fetch_list=[model.loss], return_numpy=False)\n",
    "        list_loss.append(np.array(train_loss).mean())\n",
    "    return np.mean(list_loss)\n",
    "\n",
    "def evaluate(exe, test_prog, model, test_dataset, featurizer):\n",
    "    \"\"\"\n",
    "    In the dataset, a proportion of labels are blank. So we use a `valid` tensor\n",
    "    to help eliminate these blank labels in both training and evaluation phase.\n",
    "    \n",
    "    Returns:\n",
    "        the average roc-auc of all sub-tasks.\n",
    "    \"\"\"\n",
    "    data_gen = test_dataset.iter_batch(\n",
    "    \t\tbatch_size=64, num_workers=4, shuffle=False, collate_fn=featurizer.collate_fn)\n",
    "    total_pred = []\n",
    "    total_label = []\n",
    "    total_valid = []\n",
    "    for batch_id, feed_dict in enumerate(data_gen):\n",
    "        pred, = exe.run(test_prog, feed=feed_dict, fetch_list=[model.pred], return_numpy=False)\n",
    "        total_pred.append(np.array(pred))\n",
    "        total_label.append(feed_dict['finetune_label'])\n",
    "        total_valid.append(feed_dict['valid'])\n",
    "    total_pred = np.concatenate(total_pred, 0)\n",
    "    total_label = np.concatenate(total_label, 0)\n",
    "    total_valid = np.concatenate(total_valid, 0)\n",
    "    return calc_rocauc_score(total_label, total_pred, total_valid)\n",
    "\n",
    "for epoch_id in range(4):\n",
    "    train_loss = train(exe, train_prog, model, train_dataset, featurizer)\n",
    "    val_auc = evaluate(exe, test_prog, model, valid_dataset, featurizer)\n",
    "    test_auc = evaluate(exe, test_prog, model, test_dataset, featurizer)\n",
    "    print(\"epoch:%s train/loss:%s\" % (epoch_id, train_loss))\n",
    "    print(\"epoch:%s val/auc:%s\" % (epoch_id, val_auc))\n",
    "    print(\"epoch:%s test/auc:%s\" % (epoch_id, test_auc))\n",
    "# fluid.io.save_params(exe, './model/sider', train_prog)\n",
    "fluid.io.save_params(exe, './model/tox21', train_prog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is about the finetuning steps,you can adjust as needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
