{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Pretraining and Property Prediction\n",
    "\n",
    "In recent years, with sequencing technology development, the protein sequence database scale has significantly increased. However, the cost of obtaining labeled protein sequences is still very high, as it requires biological experiments. Besides, due to the inadequate number of labeled samples, the model has a high probability of overfitting the data. Borrowing the ideas from natural language processing (NLP), we can pre-train numerous unlabeled sequences by self-supervised learning. In this way, we can extract useful biological information from proteins and transfer them to other tagged tasks to make these tasks training faster and more stable convergence. These instructions refer to the work of paper TAPE, providing the model implementation of Transformer, LSTM, and ResNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../apps/pretrained_protein/tape')\n",
    "sys.path.append('../../../')\n",
    "sys.path.append('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Related Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from utils import *\n",
    "\n",
    "# paddle.enable_static() # when paddle version >= 2.0\n",
    "\n",
    "is_distributed = False\n",
    "use_cuda = False\n",
    "thread_num = 8 # for training with cpu\n",
    "\n",
    "# Setup the execution-related parameters according to the training modes.\n",
    "exe_params = default_exe_params(is_distributed=is_distributed, use_cuda=use_cuda, thread_num=thread_num)\n",
    "exe = exe_params['exe']\n",
    "trainer_num = exe_params['trainer_num']\n",
    "trainer_id = exe_params['trainer_id']\n",
    "dist_strategy = exe_params['dist_strategy'] \n",
    "places = exe_params['places']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration Settings\n",
    "\n",
    "The network is setup according to `model_config`.\n",
    "- Task-related configurations\n",
    "    - “task”：The type of training task. Candidate task types：\n",
    "        - “pretrain\": Leverage self-supervised learning for pretraining task，for dataset `TAPE`.\n",
    "        - “classification”: Clasification task, for dataset `Remote Homology`.\n",
    "        - \"regression\": Regression task, for datasets `Fluroscence` and `Stability`.\n",
    "        - “seq_classification”: Sequence classification task, for dataset `Secondary Structure`。\n",
    "    - “class_num”: The number of class for tasks `classification` and `seq_classification`。\n",
    "    - \"label_name\": The label name in the dataset。\n",
    "- Network-related configurations\n",
    "    - “model_type\": The network type. For each network, we need to set the corresponding network hyper-parameters. We support the following networks:\n",
    "        - “transformer“\n",
    "            - ”hidden_size\"\n",
    "            - \"layer_num\"\n",
    "            - \"head_num\"\n",
    "        - \"lstm\"\n",
    "            - \"hidden_size\"\n",
    "            - \"layer_num\"\n",
    "        - \"resnet\"\n",
    "            - \"hidden_size\"\n",
    "            - \"layer_num\"\n",
    "            - \"filter_size\"\n",
    "- Other configurations (See the code for more details)\n",
    "    - “dropout_rate\"\n",
    "    - \"weight_decay\"\n",
    "    \n",
    "Following is the demo `model_config` of the task of `Secondary Structure`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_config = \\\n",
    "{\n",
    "    \"model_name\": \"secondary_structure\",\n",
    "\n",
    "    \"task\": \"seq_classification\",\n",
    "    \"class_num\": 3,\n",
    "    \"label_name\": \"labels3\",\n",
    "\n",
    "    \"model_type\": \"lstm\",\n",
    "    \"hidden_size\": 512,\n",
    "    \"layer_num\": 3,\n",
    "\n",
    "    \"comment\": \"The following hyper-parameters are optional.\",\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"weight_decay\": 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tape_model import TAPEModel # More details of the network structure are shown in tape_model.py.\n",
    "from data_gen import setup_data_loader\n",
    "\n",
    "model = TAPEModel(model_config=model_config)\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "batch_size = 32 # batch size\n",
    "train_data = './demos/secondary_structure_toy_data'\n",
    "\n",
    "train_program = fluid.Program()\n",
    "train_startup = fluid.Program()\n",
    "with fluid.program_guard(train_program, train_startup):\n",
    "    with fluid.unique_name.guard():\n",
    "        model.forward(False)\n",
    "        model.cal_loss()\n",
    "\n",
    "        # setup the optimizer\n",
    "        optimizer = default_optimizer(lr=lr, warmup_steps=0, max_grad_norm=0.1)\n",
    "        setup_optimizer(optimizer, model, use_cuda, is_distributed)\n",
    "        optimizer.minimize(model.loss)\n",
    "        \n",
    "        # setup the data loader\n",
    "        train_data_loader = setup_data_loader(\n",
    "                model,\n",
    "                model_config,\n",
    "                train_data,\n",
    "                trainer_id,\n",
    "                trainer_num,\n",
    "                places,\n",
    "                batch_size)\n",
    "        exe.run(train_startup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tExample: 10042\n",
      "\tAccuracy: 0.382892\n",
      "\tExample: 21055\n",
      "\tAccuracy: 0.423652\n",
      "\tExample: 32094\n",
      "\tAccuracy: 0.424877\n",
      "\tExample: 40606\n",
      "\tAccuracy: 0.427819\n",
      "\tExample: 49157\n",
      "\tAccuracy: 0.427243\n",
      "\tExample: 59397\n",
      "\tAccuracy: 0.430257\n",
      "\tExample: 70316\n",
      "\tAccuracy: 0.432121\n",
      "\tExample: 78011\n",
      "\tAccuracy: 0.429516\n",
      "\tExample: 86317\n",
      "\tAccuracy: 0.428861\n",
      "\tExample: 94610\n",
      "\tAccuracy: 0.429257\n",
      "\tExample: 102436\n",
      "\tAccuracy: 0.430298\n",
      "\tExample: 110738\n",
      "\tAccuracy: 0.433176\n",
      "\tExample: 119579\n",
      "\tAccuracy: 0.434190\n",
      "\tExample: 127880\n",
      "\tAccuracy: 0.434454\n",
      "\tExample: 136755\n",
      "\tAccuracy: 0.435012\n",
      "\tExample: 144800\n",
      "\tAccuracy: 0.435055\n",
      "Epoch 1\n",
      "\tExample: 10042\n",
      "\tAccuracy: 0.450607\n",
      "\tExample: 21055\n",
      "\tAccuracy: 0.459416\n",
      "\tExample: 32094\n",
      "\tAccuracy: 0.451767\n",
      "\tExample: 40606\n",
      "\tAccuracy: 0.451140\n",
      "\tExample: 49157\n",
      "\tAccuracy: 0.449132\n",
      "\tExample: 59397\n",
      "\tAccuracy: 0.449871\n",
      "\tExample: 70316\n",
      "\tAccuracy: 0.450680\n",
      "\tExample: 78011\n",
      "\tAccuracy: 0.447398\n",
      "\tExample: 86317\n",
      "\tAccuracy: 0.446204\n",
      "\tExample: 94610\n",
      "\tAccuracy: 0.446073\n",
      "\tExample: 102436\n",
      "\tAccuracy: 0.446230\n",
      "\tExample: 110738\n",
      "\tAccuracy: 0.448338\n",
      "\tExample: 119579\n",
      "\tAccuracy: 0.448900\n",
      "\tExample: 127880\n",
      "\tAccuracy: 0.449140\n",
      "\tExample: 136755\n",
      "\tAccuracy: 0.449322\n",
      "\tExample: 144800\n",
      "\tAccuracy: 0.449468\n"
     ]
    }
   ],
   "source": [
    "task = model_config['task']\n",
    "train_metric = get_metric(task)\n",
    "train_fetch_list = model.get_fetch_list()\n",
    "\n",
    "for epoch_id in range(2):\n",
    "    print('Epoch %d' % epoch_id)\n",
    "    train_metric.clear()\n",
    "    for data in train_data_loader():\n",
    "        results = exe.run(\n",
    "                program=train_program,\n",
    "                feed=data,\n",
    "                fetch_list=train_fetch_list,\n",
    "                return_numpy=False)\n",
    "        update_metric(task, train_metric, results) # update the metric\n",
    "        train_metric.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-my-env",
   "language": "python",
   "name": "python-my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
